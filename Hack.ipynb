{"cells": [{"cell_type": "code", "metadata": {"scrolled": true, "collapsed": false}, "outputs": [], "source": "!pip install pika", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "from pyspark.sql import SQLContext\n#SQLContext\nsqlContext = SQLContext(sc)\n#load rss from cloudant\ndef load():\n    hackathonb = sqlContext.read.format(\"com.cloudant.spark\").\\\n    option(\"cloudant.host\",\"<hostname>\").\\\n    option(\"cloudant.username\", \"<username>\") .\\\n    option(\"cloudant.password\",\"<password>\").\\\n    option(\"inferSchema\", \"true\").\\\n    load(\"hackathonb\")\n    return hackathonb ", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "load().show()", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.types import DoubleType\n\ndef cast_df(hackathonb):\n    cast1 = hackathonb.withColumn(\"a1\", hackathonb[\"a1\"].cast(DoubleType()))\n    cast2 = cast1.withColumn(\"a2\", cast1[\"a2\"].cast(DoubleType()))\n    cast3 = cast2.withColumn(\"a3\", cast2[\"a2\"].cast(DoubleType()))\n    cast4 = cast3.withColumn(\"a4\", cast3[\"a4\"].cast(DoubleType()))\n    return cast4\n\n#cast4.printSchema()\ndef normarize(inpul):\n    assembler = VectorAssembler(inputCols=[\"a1\",\"a2\",\"a3\",\"a4\"], outputCol=\"features\")\n    feature_vectors = assembler.transform(inpul)\n    return feature_vectors.select([\"uname\",\"a1\",\"a2\",\"a3\",\"a4\",\"features\"])", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "from pyspark.ml.feature import StandardScaler\n\ndef standardization(inpul):\n    scaler = StandardScaler(inputCol=\"features\", outputCol=\"standardization\", withStd=True, withMean=True)\n    scalerModel = scaler.fit(inpul)\n    return scalerModel.transform(inpul)\n\n#\u6a19\u6e96\u5316\u5909\u91cf\u3060\u3051\u8868\u793a\n#print(\"==== \u6a19\u6e96\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf ====\")\n#std_feature_vectors.select(\"standardization\").show(truncate=False)", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "from pyspark.ml.feature import PCA\n\ndef cPCA(inpul):\n    pca = PCA(k=2, inputCol=\"standardization\", outputCol=\"pca\")\n    return pca.fit(inpul)\n\n#print(\"==== \u56fa\u6709\u30d9\u30af\u30c8\u30eb ====\")\n#print(pcaModel.pc)\n\n#print(\"==== \u5bc4\u4e0e\u7387 ====\")\n#print(pcaModel.explainedVariance)\n\n#pca_score = pcaModel.transform(std_feature_vectors).select([\"uname\",\"pca\"])\n#print(\"==== \u4e3b\u6210\u5206\u5f97\u70b9 ====\")\n#pca_score.show(truncate=False)", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "import math\ndef sigmoid(z):\n    return 1/(1+math.e**(-z)) ", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "import pika  \nimport sys  \nimport ssl\n\ntry:  \n    from urllib import urlencode\nexcept ImportError:  \n    from urllib.parse import urlencode\n    \ndef publish(data):\n    ssl_opts = \"<rabbitmq_sslopts>\"\n    full_url='amqps://<rabbitmq_user>:<rabbitmq_password>@<rabbitmq_host>?'+ssl_opts  \n    parameters = pika.URLParameters(full_url)\n    connection = pika.BlockingConnection(parameters)  \n    channel = connection.channel()\n    message='[' + data + ']'  \n    my_routing_key='hello'  \n    channel.basic_publish(exchange='',  \n                      routing_key=my_routing_key,\n                      body=message)\n    channel.close()  \n    connection.close()  ", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "cloudant_df = load()\ninput_df = cast_df(cloudant_df)\nt_input_df = normarize(input_df)\nsta_df = standardization(t_input_df)\nmodel = cPCA(sta_df)\nmodel.transform(sta_df).select([\"uname\",\"pca\"]).show(truncate=False)\nlst_res = model.transform(sta_df).select([\"uname\",\"pca\"]).rdd.collect()", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "import time\ndef run():\n    print (\"run\")\n    cloudant_df = load()\n    input_df = cast_df(cloudant_df)\n    t_input_df = normarize(input_df)\n    sta_df = standardization(t_input_df)\n    model = cPCA(sta_df)\n    lst_res = model.transform(sta_df).select([\"uname\",\"pca\"]).rdd.collect()\n    lst_sorted = sorted(lst_res)\n    str_list = [\"{\\\"lbl\\\":\\\"\" + x.uname + \"\\\",\\\"val\\\":[\" + str(sigmoid(x.pca[0]+((i+1) * 1/11)) * 700) + \",\" + str(sigmoid(x.pca[1]+((i+1) * 1/10)) * 700) + \"]}\" for (i,x) in enumerate(lst_sorted)]\n    publish(','.join(str_list))\n    [print(x.pca) for x in lst_res]\nrun()   \n", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "import time\ndef run():\n    print (\"run\")\n    cloudant_df = load()\n    input_df = cast_df(cloudant_df)\n    t_input_df = normarize(input_df)\n    sta_df = standardization(t_input_df)\n    model = cPCA(sta_df)\n    lst_res = model.transform(sta_df).select([\"uname\",\"pca\"]).rdd.collect()\n    lst_sorted = sorted(lst_res)\n    str_list = [\"{\\\"lbl\\\":\\\"\" + x.uname + \"\\\",\\\"val\\\":[\" + str(sigmoid(x.pca[0]+((i+1) * 1/11)) * 700) + \",\" + str(sigmoid(x.pca[1]+((i+1) * 1/10)) * 700) + \"]}\" for (i,x) in enumerate(lst_sorted)]\n    #print(str_list)\n    publish(','.join(str_list))\n    #[print(x.pca) for x in lst_res]\n#run()   \nwhile True:\n    run()\n    time.sleep(1)", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "", "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "", "execution_count": null}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3.5 (Experimental) with Spark 2.0", "name": "python3-spark20", "language": "python"}, "language_info": {"version": "3.5.2", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3"}}, "nbformat_minor": 0}